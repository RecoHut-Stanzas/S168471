{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GroupIM: A Mutual Information Maximization Framework for Neural Group Recommendation\n",
        "\n",
        "## Executive summary\n",
        "\n",
        "| | |\n",
        "| --- | --- |\n",
        "| Problem | Group interactions are sparse in nature which makes it difficult to provide relevant recommendation to the group. |\n",
        "| Solution | Regularize the user-group latent space to overcome group interaction sparsity by: maximizing mutual information between representations of groups and group members; and dynamically prioritizing the preferences of highly informative members through contextual preference weighting. |\n",
        "| Dataset | Weeplaces |\n",
        "| Preprocessing | We extract check-ins on POIs over all major cities in the United States, across various categories including Food, Nightlife, Outdoors, Entertainment and Travel. We randomly split the set of all groups into training (70%), validation (10%), and test (20%) sets, while utilizing the individual interactions of all users for training. Note that each group appears only in one of the three sets. The test set contains strict ephemeral groups (i.e., a specific combination of users) that do not occur in the training set. Thus, we train on ephemeral groups and test on strict ephemeral groups. |\n",
        "| Metrics | NDCG, Recall |\n",
        "| Hyperparams | We tune the latent dimension in the range {32, 64, 128} and other baseline hyper-parameters in ranges centered at author-provided values. In GroupIM, we use two fully connected layers of size 64 each in fenc(·) and tune λ in the range {$2^{−4}$,$2^{−3}$,$\\dots$, $2^{6}$}. We use 5 negatives for each true user-group pair to train the discriminator. |\n",
        "| Models | GroupIM along with Encoder, 3 types of Aggregators to choose from, and a discriminator module. |\n",
        "| Platform | PyTorch, preferable GPU for faster computation. |\n",
        "| Links | [Paper](https://arxiv.org/abs/2006.03736), [Code](https://github.com/RecoHut-Stanzas/S168471) |\n",
        "\n",
        "## Weeplaces dataset\n",
        "\n",
        "This dataset is collected from Weeplaces, a website that aims to visualize users’ check-in activities in location-based social networks (LBSN). It is now integrated with the APIs of other location-based social networking services, e.g., Facebook Places, Foursquare, and Gowalla. Users can login Weeplaces using their LBSN accounts and connect with their friends in the same LBSN who have also used this application. All the crawled data is originally generated in Foursquare. This dataset contains 7,658,368 check-ins generated by 15,799 users over 971,309 locations. In the data collection, we can’t get the original Foursquare IDs of the Weeplaces users. We can only get their check-in history, their friends who also use Weeplaces, and other additional information about the locations.\n",
        "\n",
        "You can download this dataset from **[here](https://drive.google.com/file/d/0BzpKyxX1dqTYYzRmUXRZMWloblU/view?usp=sharing)** (about 140 MB). Note that this dataset is released solely for research purpose.\n",
        "\n",
        "## Group Recommendation Systems\n",
        "\n",
        "This line of work can be divided into two categories based on group types: persistent and ephemeral. Persistent groups have stable members with rich activity history together, while ephemeral groups comprise users who interact with very few items together. A common approach is to consider persistent groups as virtual users, thus, personalized recommenders can be directly applied. However, such methods cannot handle ephemeral groups with sparse interactions. \n",
        "\n",
        "Prior work either aggregate recommendation results (or item scores) for each member, or aggregate individual member preferences, towards group predictions. They fall into two classes: score (or late) aggregation and preference (or early) aggregation. Popular score aggregation strategies include least misery, average, maximum satisfaction, and relevance and disagreement. However, these are hand-crafted heuristics that overlook real-world group interactions. Baltrunas et al. compare different strategies to conclude that there is no clear winner, and their relative effectiveness depends on group size and group coherence. Early preference aggregation strategies generate recommendations by constructing a group profile that combines the profiles (raw item histories) of group members. Recent methods adopt a model-based perspective to learn data-driven models. Probabilistic methods model the group generative process by considering both the personal preferences and relative influence of members, to differentiate their contributions towards group decisions. However, a key weakness is their assumption that users have the same likelihood to follow individual and collective preferences, across different groups. Neural methods explore attention mechanisms to learn data-driven preference aggregators. MoSAN models group interactions via sub-attention networks; however, MoSAN operates on persistent groups while ignoring users’ personal activities. AGREE employs attentional networks for joint training over individual and group interactions; yet, the extent of regularization applied on each user (based on personal activities) is the same across groups, which results in degenerate solutions when applied to ephemeral groups with sparse activities. An alternative approach to tackle interaction sparsity is to exploit external side information, e.g., social network of users, personality traits, demographics, and interpersonal relationships.\n",
        "\n",
        "## Tutorials\n",
        "\n",
        "### Training GroupIM model (PyTorch) on Weeplaces dataset\n",
        "\n",
        "[direct link to notebook →](https://github.com/RecoHut-Stanzas/S168471/blob/main/nbs/P042235_Training_GroupIM_model_(PyTorch)_on_Weeplaces_dataset.ipynb)\n",
        "\n",
        "In this notebook, we are first fetching a preprocessed version of the Weeplaces dataset from the *RecoHut-Datasets* server. Then we are building a dataset class module inheriting from the pytorch’s Dataset. We then implement GroupIM model along with Encoder, 3 types of Aggregators to choose from, and a discriminator module. All these modules are inherited from the pytorch’s `nn.Module`. Then we implement metrics and evaluation functions and train the model.\n",
        "\n",
        "## References\n",
        "\n",
        "1. [https://arxiv.org/abs/2006.03736](https://arxiv.org/abs/2006.03736)\n",
        "2. [https://github.com/RecoHut-Stanzas/S168471](https://github.com/RecoHut-Stanzas/S168471)"
      ],
      "metadata": {
        "id": "lIYdn1woOS1n"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}